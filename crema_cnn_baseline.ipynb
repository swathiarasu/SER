{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64c87c15",
   "metadata": {},
   "source": [
    "\n",
    "# CREMA-D - CNN Baseline\n",
    "\n",
    "This notebook implements a lightweight, interpretable CNN baseline on **CREMA-D** using 128-band log-mel spectrograms.\n",
    "\n",
    "**What it does**\n",
    "- Loads CREMA-D `.wav` files from a path you set (`DATA_ROOT`)\n",
    "- Extracts 128-mel log spectrograms (resampled to 16 kHz)\n",
    "- 4 convolutional blocks (32->64->128->256) + dense with dropout\n",
    "- Adam optimizer, early stopping, simple augmentations (time shift, noise, SpecAugment)\n",
    "- Stratified train/val/test split and final report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240f4bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: install dependencies (run locally if needed)\n",
    "# %pip install torch torchaudio librosa scikit-learn numpy matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23d8282",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, random\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Config\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DATA_ROOT = \"/Users/swathiarasu/Desktop/Courses/Sem 3/CS 7150/Emotion Detection/AudioWAV\"  \n",
    "\n",
    "SAMPLE_RATE = 16_000\n",
    "N_MELS = 128\n",
    "N_FFT = 1024\n",
    "HOP_LENGTH = 256\n",
    "WIN_LENGTH = 1024\n",
    "FMIN = 20\n",
    "FMAX = 8000\n",
    "DURATION = 2.5     \n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-3\n",
    "EPOCHS = 50\n",
    "PATIENCE = 6          # early stopping\n",
    "VAL_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "EMO_MAP = {\"ANG\":0, \"DIS\":1, \"FEA\":2, \"HAP\":3, \"NEU\":4, \"SAD\":5}\n",
    "INV_EMO = {v:k for k,v in EMO_MAP.items()}\n",
    "\n",
    "# Utils\n",
    "\n",
    "def parse_label_from_filename(fname: str):\n",
    "    base = Path(fname).stem\n",
    "    parts = base.split(\"_\")\n",
    "    emo = parts[2] if len(parts) > 2 else None\n",
    "    if emo not in EMO_MAP:\n",
    "        return None\n",
    "    return EMO_MAP[emo]\n",
    "\n",
    "def load_wav_centered(path, target_sr=SAMPLE_RATE, duration=DURATION):\n",
    "    wav, sr = librosa.load(path, sr=target_sr, mono=True)\n",
    "    target_len = int(duration * target_sr)\n",
    "    if len(wav) >= target_len:\n",
    "        start = (len(wav) - target_len) // 2\n",
    "        wav = wav[start:start+target_len]\n",
    "    else:\n",
    "        pad = target_len - len(wav)\n",
    "        left = pad // 2\n",
    "        right = pad - left\n",
    "        wav = np.pad(wav, (left, right), mode=\"reflect\")\n",
    "    return wav\n",
    "\n",
    "def wav_to_logmel(wav, sr=SAMPLE_RATE):\n",
    "    S = librosa.feature.melspectrogram(\n",
    "        y=wav, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH, win_length=WIN_LENGTH,\n",
    "        n_mels=N_MELS, fmin=FMIN, fmax=FMAX, power=2.0\n",
    "    )\n",
    "    logS = librosa.power_to_db(S, ref=np.max)\n",
    "    # per-utterance standardization\n",
    "    mean = logS.mean()\n",
    "    std = logS.std() + 1e-6\n",
    "    logS = (logS - mean) / std\n",
    "    return logS.astype(np.float32)\n",
    "\n",
    "# Augmentations\n",
    "def augment_waveform(wav, sr=SAMPLE_RATE, p_shift=0.5, p_noise=0.5):\n",
    "    # time shift\n",
    "    if random.random() < p_shift:\n",
    "        max_shift = int(0.1 * len(wav))\n",
    "        shift = random.randint(-max_shift, max_shift)\n",
    "        wav = np.roll(wav, shift)\n",
    "    # add light noise\n",
    "    if random.random() < p_noise:\n",
    "        noise_amp = 0.005 * np.random.uniform() * (np.amax(np.abs(wav)) + 1e-6)\n",
    "        wav = wav + noise_amp * np.random.normal(size=wav.shape[0])\n",
    "    return wav\n",
    "\n",
    "def spec_augment(mel, num_time_masks=1, num_freq_masks=1, max_time_mask=0.1, max_freq_mask=0.1):\n",
    "    m = mel.copy()\n",
    "    n_mels, T = m.shape\n",
    "    # freq masks\n",
    "    for _ in range(num_freq_masks):\n",
    "        f = int(max(1, max_freq_mask * n_mels * np.random.rand()))\n",
    "        f0 = random.randint(0, max(0, n_mels - f))\n",
    "        m[f0:f0+f, :] = 0.0\n",
    "    # time masks\n",
    "    for _ in range(num_time_masks):\n",
    "        t = int(max(1, max_time_mask * T * np.random.rand()))\n",
    "        t0 = random.randint(0, max(0, T - t))\n",
    "        m[:, t0:t0+t] = 0.0\n",
    "    return m\n",
    "\n",
    "# Dataset\n",
    "\n",
    "class CremaMelDataset(Dataset):\n",
    "    def __init__(self, files, train=True):\n",
    "        self.files = files\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self): return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fpath = self.files[idx]\n",
    "        y = parse_label_from_filename(fpath)\n",
    "        wav = load_wav_centered(fpath, SAMPLE_RATE, DURATION)\n",
    "        if self.train:\n",
    "            wav = augment_waveform(wav, SAMPLE_RATE, p_shift=0.7, p_noise=0.7)\n",
    "        mel = wav_to_logmel(wav, SAMPLE_RATE)\n",
    "        if self.train:\n",
    "            mel = spec_augment(mel, num_time_masks=1, num_freq_masks=1)\n",
    "        mel = torch.from_numpy(mel)[None, :, :]  # (1, n_mels, T)\n",
    "        return mel, y\n",
    "\n",
    "# Model: 4 conv blocks \n",
    "\n",
    "class CNNBaseline(nn.Module):\n",
    "    def __init__(self, n_classes=6, in_ch=1, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc1 = nn.Linear(256, 256)\n",
    "        self.drop_fc = nn.Dropout(0.5)\n",
    "        self.out = nn.Linear(256, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.gap(x).squeeze(-1).squeeze(-1)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop_fc(x)\n",
    "        return self.out(x)\n",
    "\n",
    "# Early Stopping\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=PATIENCE, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best = None\n",
    "        self.count = 0\n",
    "        self.stopped = False\n",
    "\n",
    "    def step(self, metric):\n",
    "        if self.best is None or metric < self.best - self.min_delta:\n",
    "            self.best = metric\n",
    "            self.count = 0\n",
    "        else:\n",
    "            self.count += 1\n",
    "            if self.count >= self.patience:\n",
    "                self.stopped = True\n",
    "        return self.stopped\n",
    "\n",
    "# Train / Eval loops\n",
    "\n",
    "def train_one_epoch(model, loader, opt, criterion):\n",
    "    model.train()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(DEVICE), torch.tensor(yb).to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_sum += loss.item() * xb.size(0)\n",
    "        preds = logits.argmax(1)\n",
    "        total += yb.size(0)\n",
    "        correct += (preds == yb).sum().item()\n",
    "    return loss_sum/total, correct/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(DEVICE), torch.tensor(yb).to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss_sum += loss.item() * xb.size(0)\n",
    "        preds = logits.argmax(1)\n",
    "        total += yb.size(0)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(yb.cpu().numpy())\n",
    "    import numpy as np\n",
    "    avg_loss = loss_sum/total\n",
    "    acc = correct/total\n",
    "    return avg_loss, acc, np.concatenate(all_preds), np.concatenate(all_labels)\n",
    "\n",
    "def collect_files(root):\n",
    "    wavs = sorted(glob(os.path.join(root, \"*.wav\")))\n",
    "    files = [f for f in wavs if parse_label_from_filename(f) is not None]\n",
    "    return files\n",
    "\n",
    "def stratified_split(files):\n",
    "    labels = [parse_label_from_filename(f) for f in files]\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        files, labels, test_size=VAL_SPLIT+TEST_SPLIT, random_state=SEED, stratify=labels\n",
    "    )\n",
    "    rel = (VAL_SPLIT+TEST_SPLIT)\n",
    "    val_ratio = VAL_SPLIT / rel\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=1 - val_ratio, random_state=SEED, stratify=y_temp\n",
    "    )\n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "def main():\n",
    "    files = collect_files(DATA_ROOT)\n",
    "    print(f\"Found {len(files)} usable wav files.\")\n",
    "    if len(files) == 0:\n",
    "        raise FileNotFoundError(\"No usable .wav files found. Check DATA_ROOT and filename format.\")\n",
    "    X_train, X_val, X_test = stratified_split(files)\n",
    "    print(f\"Split sizes -> train: {len(X_train)} | val: {len(X_val)} | test: {len(X_test)}\")\n",
    "\n",
    "    train_ds = CremaMelDataset(X_train, train=True)\n",
    "    val_ds   = CremaMelDataset(X_val,   train=False)\n",
    "    test_ds  = CremaMelDataset(X_test,  train=False)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    model = CNNBaseline(n_classes=len(EMO_MAP)).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    history = {\n",
    "        \"epoch\":      [],\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\":   [],\n",
    "        \"train_acc\":  [],\n",
    "        \"val_acc\":    [],\n",
    "    }\n",
    "\n",
    "    stopper = EarlyStopper(patience=PATIENCE, min_delta=0.0)\n",
    "    best_state = None\n",
    "    best_val = float(\"inf\")\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        va_loss, va_acc, _, _ = evaluate(model, val_loader, criterion)\n",
    "        print(f\"Epoch {epoch:02d}: train_loss={tr_loss:.4f} acc={tr_acc:.3f} | val_loss={va_loss:.4f} acc={va_acc:.3f}\")\n",
    "\n",
    "        history[\"epoch\"].append(epoch)\n",
    "        history[\"train_loss\"].append(tr_loss)\n",
    "        history[\"val_loss\"].append(va_loss)\n",
    "        history[\"train_acc\"].append(tr_acc)\n",
    "        history[\"val_acc\"].append(va_acc)\n",
    "\n",
    "        if va_loss < best_val:\n",
    "            best_val = va_loss\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "        if stopper.step(va_loss):\n",
    "            print(f\"Early stopping at epoch {epoch}.\")\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    pd.DataFrame(history).to_csv(\"cnn_history.csv\", index=False)\n",
    "    print(\"Saved cnn_history.csv\")\n",
    "\n",
    "    # final test evaluation\n",
    "    te_loss, te_acc, preds, labels = evaluate(model, test_loader, criterion)\n",
    "    print(f\"\\nTest: loss={te_loss:.4f} acc={te_acc:.3f}\")\n",
    "    print(classification_report(labels, preds, target_names=[INV_EMO[i] for i in range(len(EMO_MAP))]))\n",
    "    report = classification_report(labels, preds, target_names=[INV_EMO[i] for i in range(len(EMO_MAP))])\n",
    "    with open(\"classification_report_cnn.txt\", \"w\") as f:\n",
    "        f.write(report)\n",
    "    print(\"Saved CNN classification report to classification_report_cnn.txt\")\n",
    "\n",
    "    # export test predictions \n",
    "    pd.DataFrame({\"y_true\": labels, \"y_pred\": preds}).to_csv(\"cnn_preds.csv\", index=False)\n",
    "    print(\"Saved cnn_preds.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bf2a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7442 usable wav files.\n",
      "Split sizes -> train: 5953 | val: 744 | test: 745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hx/g3w2fmcs4s72f1j2q62mkl100000gn/T/ipykernel_5881/146498213.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xb, yb = xb.to(DEVICE), torch.tensor(yb).to(DEVICE)\n",
      "/var/folders/hx/g3w2fmcs4s72f1j2q62mkl100000gn/T/ipykernel_5881/146498213.py:225: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xb, yb = xb.to(DEVICE), torch.tensor(yb).to(DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: train_loss=1.6069 acc=0.311 | val_loss=1.5597 acc=0.360\n",
      "Epoch 02: train_loss=1.5212 acc=0.363 | val_loss=1.7022 acc=0.293\n",
      "Epoch 03: train_loss=1.4967 acc=0.371 | val_loss=1.4973 acc=0.368\n",
      "Epoch 04: train_loss=1.4908 acc=0.378 | val_loss=1.4192 acc=0.423\n",
      "Epoch 05: train_loss=1.4619 acc=0.380 | val_loss=1.4355 acc=0.456\n",
      "Epoch 06: train_loss=1.4405 acc=0.401 | val_loss=1.3955 acc=0.409\n",
      "Epoch 07: train_loss=1.4167 acc=0.414 | val_loss=1.4663 acc=0.398\n",
      "Epoch 08: train_loss=1.4022 acc=0.421 | val_loss=1.3734 acc=0.442\n",
      "Epoch 09: train_loss=1.3825 acc=0.424 | val_loss=1.5394 acc=0.395\n",
      "Epoch 10: train_loss=1.3848 acc=0.431 | val_loss=1.3919 acc=0.383\n",
      "Epoch 11: train_loss=1.3758 acc=0.440 | val_loss=1.4428 acc=0.402\n",
      "Epoch 12: train_loss=1.3700 acc=0.437 | val_loss=1.5605 acc=0.379\n",
      "Epoch 13: train_loss=1.3552 acc=0.447 | val_loss=1.2635 acc=0.491\n",
      "Epoch 14: train_loss=1.3465 acc=0.454 | val_loss=1.3451 acc=0.438\n",
      "Epoch 15: train_loss=1.3415 acc=0.462 | val_loss=1.2612 acc=0.515\n",
      "Epoch 16: train_loss=1.3256 acc=0.463 | val_loss=1.4723 acc=0.378\n",
      "Epoch 17: train_loss=1.3316 acc=0.471 | val_loss=1.2778 acc=0.454\n",
      "Epoch 18: train_loss=1.3184 acc=0.470 | val_loss=1.4153 acc=0.406\n",
      "Epoch 19: train_loss=1.3060 acc=0.482 | val_loss=1.2320 acc=0.500\n",
      "Epoch 20: train_loss=1.3206 acc=0.484 | val_loss=1.3249 acc=0.452\n",
      "Epoch 21: train_loss=1.2970 acc=0.481 | val_loss=1.3051 acc=0.499\n",
      "Epoch 22: train_loss=1.2852 acc=0.486 | val_loss=1.2700 acc=0.462\n",
      "Epoch 23: train_loss=1.2801 acc=0.493 | val_loss=1.6144 acc=0.406\n",
      "Epoch 24: train_loss=1.2704 acc=0.496 | val_loss=1.2117 acc=0.523\n",
      "Epoch 25: train_loss=1.2590 acc=0.509 | val_loss=1.4060 acc=0.466\n",
      "Epoch 26: train_loss=1.2661 acc=0.511 | val_loss=1.4898 acc=0.414\n",
      "Epoch 27: train_loss=1.2714 acc=0.502 | val_loss=1.4249 acc=0.427\n",
      "Epoch 28: train_loss=1.2557 acc=0.514 | val_loss=1.5978 acc=0.363\n",
      "Epoch 29: train_loss=1.2354 acc=0.509 | val_loss=1.4757 acc=0.387\n",
      "Epoch 30: train_loss=1.2450 acc=0.509 | val_loss=1.3015 acc=0.492\n",
      "Early stopping at epoch 30.\n",
      "Saved cnn_history.csv\n",
      "\n",
      "Test: loss=1.2384 acc=0.530\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ANG       0.76      0.51      0.61       127\n",
      "         DIS       0.39      0.59      0.47       127\n",
      "         FEA       0.59      0.40      0.48       127\n",
      "         HAP       0.45      0.60      0.52       128\n",
      "         NEU       0.74      0.29      0.42       109\n",
      "         SAD       0.57      0.75      0.64       127\n",
      "\n",
      "    accuracy                           0.53       745\n",
      "   macro avg       0.58      0.52      0.52       745\n",
      "weighted avg       0.58      0.53      0.53       745\n",
      "\n",
      "Saved CNN classification report to classification_report_cnn.txt\n",
      "Saved cnn_preds.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
